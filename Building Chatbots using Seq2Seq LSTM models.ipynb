{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './conversation_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len from: 265, len to: 265\n"
     ]
    }
   ],
   "source": [
    "with open(file_path+'from.txt', 'r') as fopen:\n",
    "    text_from = fopen.read().lower().split('\\n')\n",
    "with open(file_path+'to.txt', 'r') as fopen:\n",
    "    text_to = fopen.read().lower().split('\\n')\n",
    "print('len from: %d, len to: %d'%(len(text_from), len(text_to)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good morning', 'good afternoon', 'good evening', 'good night', 'how are you', 'how are you doing', 'what is your name', 'whats your name', 'may i have your name']\n"
     ]
    }
   ],
   "source": [
    "print(text_from[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good morning', 'good afternoon', 'good evening', 'good night have a nice dream', 'i am fine thank you', 'doing good thank you', 'my name is papaya and what do you want me to call you dear sir or madam', 'my name is papaya may i also have your name please', 'sure my name is papaya may i also have your name please']\n"
     ]
    }
   ],
   "source": [
    "print(text_to[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 331\n",
      "Most common words [('you', 73), ('is', 67), ('what', 63), ('a', 49), ('the', 40), ('do', 36)]\n",
      "Sample data [92, 57, 328, 57, 323, 57, 325, 57, 204, 11] ['hi', 'good', 'morning', 'good', 'afternoon', 'good', 'evening', 'good', 'night', 'how']\n"
     ]
    }
   ],
   "source": [
    "concat_from = ' '.join(text_from).split()\n",
    "vocabulary_size_from = len(list(set(concat_from)))\n",
    "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
    "print('vocab from size: %d'%(vocabulary_size_from))\n",
    "print('Most common words', count_from[4:10])\n",
    "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab to size: 504\n",
      "Most common words [('i', 127), ('you', 55), ('a', 47), ('to', 44), ('the', 40), ('it', 38)]\n",
      "Sample data [171, 137, 37, 481, 37, 354, 37, 356, 37, 419] ['hi', 'there', 'good', 'morning', 'good', 'afternoon', 'good', 'evening', 'good', 'night']\n"
     ]
    }
   ],
   "source": [
    "concat_to = ' '.join(text_to).split()\n",
    "vocabulary_size_to = len(list(set(concat_to)))\n",
    "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
    "print('vocab to size: %d'%(vocabulary_size_to))\n",
    "print('Most common words', count_to[4:10])\n",
    "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: defining a seq2seq model that has 4 components: embedding layer, encoder, decoder, cost & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size,\n",
    "                 from_dict_size, to_dict_size, learning_rate, batch_size):\n",
    "        \n",
    "        def cells(reuse=False):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_seq_len = tf.placeholder(tf.int32, [None])\n",
    "        self.Y_seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        with tf.variable_scope(\"encoder_embeddings\"):        \n",
    "            \n",
    "            encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n",
    "            encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "            main = tf.strided_slice(self.X, [0, 0], [batch_size, -1], [1, 1])\n",
    "            \n",
    "        with tf.variable_scope(\"decoder_embeddings\"):        \n",
    "            decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "            decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
    "            decoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, decoder_input)\n",
    "        \n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n",
    "            _, last_state = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded,\n",
    "                                              dtype = tf.float32)\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            rnn_cells_dec = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n",
    "            outputs, _ = tf.nn.dynamic_rnn(rnn_cells_dec, decoder_embedded, \n",
    "                                           initial_state = last_state,\n",
    "                                           dtype = tf.float32)\n",
    "        with tf.variable_scope(\"logits\"):            \n",
    "            self.logits = tf.layers.dense(outputs,to_dict_size)\n",
    "            print(self.logits)\n",
    "            masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        with tf.variable_scope(\"cost\"):            \n",
    "            self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.logits,\n",
    "                                                         targets = self.Y,\n",
    "                                                         weights = masks)\n",
    "        with tf.variable_scope(\"optimizer\"):            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3; #Hyperparameters         \n",
    "            \n",
    "size_layer = 128\n",
    "num_layers = 2\n",
    "embedded_size = 128\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epoch =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"logits/dense/BiasAdd:0\", shape=(32, ?, 508), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Chatbot(size_layer, num_layers, embedded_size, vocabulary_size_from + 4, \n",
    "                vocabulary_size_to + 4, learning_rate, batch_size)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=2)\n",
    "checkpoint_dir = os.path.abspath(os.path.join('./', \"checkpoints_chatbot\"))\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dude'\n",
      "'plan'\n"
     ]
    }
   ],
   "source": [
    "def str_idx(corpus, dic):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i.split():\n",
    "            try:\n",
    "                ints.append(dic[k])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                ints.append(2)\n",
    "        X.append(ints)\n",
    "    return X\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = 50\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(50)\n",
    "    return padded_seqs, seq_lens\n",
    "\n",
    "def check_accuracy(logits, Y):\n",
    "    acc = 0\n",
    "    for i in range(logits.shape[0]):\n",
    "        internal_acc = 0\n",
    "        for k in range(len(Y[i])):\n",
    "            if Y[i][k] == logits[i][k]:\n",
    "                internal_acc += 1\n",
    "        acc += (internal_acc / len(Y[i]))\n",
    "    return acc / logits.shape[0]\n",
    "\n",
    "X = str_idx(text_from, dictionary_from)\n",
    "Y = str_idx(text_to, dictionary_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg loss: 4.562975, avg accuracy: 0.754766\n",
      "epoch: 2, avg loss: 1.644718, avg accuracy: 0.865859\n",
      "epoch: 3, avg loss: 1.016150, avg accuracy: 0.865859\n",
      "epoch: 4, avg loss: 0.930139, avg accuracy: 0.865859\n",
      "epoch: 5, avg loss: 0.889850, avg accuracy: 0.865859\n",
      "epoch: 6, avg loss: 0.873128, avg accuracy: 0.865859\n",
      "epoch: 7, avg loss: 0.863021, avg accuracy: 0.868203\n",
      "epoch: 8, avg loss: 0.855188, avg accuracy: 0.869297\n",
      "epoch: 9, avg loss: 0.848771, avg accuracy: 0.869844\n",
      "epoch: 10, avg loss: 0.843564, avg accuracy: 0.870781\n",
      "epoch: 11, avg loss: 0.838772, avg accuracy: 0.870781\n",
      "epoch: 12, avg loss: 0.834623, avg accuracy: 0.870781\n",
      "epoch: 13, avg loss: 0.830765, avg accuracy: 0.870781\n",
      "epoch: 14, avg loss: 0.827309, avg accuracy: 0.870781\n",
      "epoch: 15, avg loss: 0.823929, avg accuracy: 0.870781\n",
      "epoch: 16, avg loss: 0.820607, avg accuracy: 0.870781\n",
      "epoch: 17, avg loss: 0.817260, avg accuracy: 0.870938\n",
      "epoch: 18, avg loss: 0.813865, avg accuracy: 0.870781\n",
      "epoch: 19, avg loss: 0.810353, avg accuracy: 0.870781\n",
      "epoch: 20, avg loss: 0.806720, avg accuracy: 0.870703\n",
      "epoch: 21, avg loss: 0.802785, avg accuracy: 0.870703\n",
      "epoch: 22, avg loss: 0.798037, avg accuracy: 0.870703\n",
      "epoch: 23, avg loss: 0.792071, avg accuracy: 0.870703\n",
      "epoch: 24, avg loss: 0.785556, avg accuracy: 0.870703\n",
      "epoch: 25, avg loss: 0.781322, avg accuracy: 0.870781\n",
      "epoch: 26, avg loss: 0.774452, avg accuracy: 0.870703\n",
      "epoch: 27, avg loss: 0.770596, avg accuracy: 0.870781\n",
      "epoch: 28, avg loss: 0.766712, avg accuracy: 0.870703\n",
      "epoch: 29, avg loss: 0.767071, avg accuracy: 0.870781\n",
      "epoch: 30, avg loss: 0.778540, avg accuracy: 0.870859\n",
      "epoch: 31, avg loss: 0.784142, avg accuracy: 0.870859\n",
      "epoch: 32, avg loss: 0.775102, avg accuracy: 0.870781\n",
      "epoch: 33, avg loss: 0.762762, avg accuracy: 0.870938\n",
      "epoch: 34, avg loss: 0.755581, avg accuracy: 0.870781\n",
      "epoch: 35, avg loss: 0.745892, avg accuracy: 0.870859\n",
      "epoch: 36, avg loss: 0.744793, avg accuracy: 0.871016\n",
      "epoch: 37, avg loss: 0.743298, avg accuracy: 0.871094\n",
      "epoch: 38, avg loss: 0.738738, avg accuracy: 0.871094\n",
      "epoch: 39, avg loss: 0.732203, avg accuracy: 0.871016\n",
      "epoch: 40, avg loss: 0.729088, avg accuracy: 0.871016\n",
      "epoch: 41, avg loss: 0.726649, avg accuracy: 0.871094\n",
      "epoch: 42, avg loss: 0.726724, avg accuracy: 0.871562\n",
      "epoch: 43, avg loss: 0.730131, avg accuracy: 0.871641\n",
      "epoch: 44, avg loss: 0.717863, avg accuracy: 0.871250\n",
      "epoch: 45, avg loss: 0.711672, avg accuracy: 0.871484\n",
      "epoch: 46, avg loss: 0.709344, avg accuracy: 0.871875\n",
      "epoch: 47, avg loss: 0.708003, avg accuracy: 0.872031\n",
      "epoch: 48, avg loss: 0.713312, avg accuracy: 0.872109\n",
      "epoch: 49, avg loss: 0.714219, avg accuracy: 0.872031\n",
      "epoch: 50, avg loss: 0.716421, avg accuracy: 0.872031\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    for k in range(0, (len(text_from) // batch_size) * batch_size, batch_size):\n",
    "        batch_x, seq_x = pad_sentence_batch(X[k: k+batch_size], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(Y[k: k+batch_size], PAD)\n",
    "        predicted, loss, _ = sess.run([tf.argmax(model.logits,2), model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y,\n",
    "                                                model.X_seq_len:seq_x,\n",
    "                                                model.Y_seq_len:seq_y})\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_accuracy += check_accuracy(predicted,batch_y)\n",
    "#        print 'output:', [rev_dictionary_to[i] for i in predicted[0]]\n",
    "#        print 'input:', [rev_dictionary_to[i] for i in batch_x[0]]\n",
    "        \n",
    "    total_loss /= (len(text_from) // batch_size)\n",
    "    total_accuracy /= (len(text_from) // batch_size)\n",
    "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, total_loss, total_accuracy))\n",
    "    path = saver.save(sess, checkpoint_prefix, global_step=i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 5: evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/stephen/NLP/Python-Deep-Learning-Projects/Chapter05/checkpoints_chatbot/model-50\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    X_in = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            X_in.append(dictionary_from[word])\n",
    "        except:\n",
    "            X_in.append(PAD)\n",
    "            pass\n",
    "        \n",
    "    test, seq_x = pad_sentence_batch([X_in], PAD)\n",
    "    input_batch = np.zeros([batch_size,seq_x[0]])\n",
    "    input_batch[0] =test[0] \n",
    "        \n",
    "    log = sess.run(tf.argmax(model.logits,2), \n",
    "                                      feed_dict={\n",
    "                                              model.X:input_batch,\n",
    "                                              model.X_seq_len:seq_x,\n",
    "                                              model.Y_seq_len:seq_x\n",
    "                                              }\n",
    "                                      )\n",
    "    \n",
    "    result=' '.join(rev_dictionary_to[i] for i in log[0])\n",
    "    return result\n",
    "    \n",
    "checkpoint_file = tf.train.latest_checkpoint(os.path.join('./', 'checkpoints_chatbot'))\n",
    "saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "saver.restore(sess, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "#50 epochs\n",
    "print (predict('where do you live?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i i PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "print (predict('how are you doing ?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i i PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "print (predict('how are you ?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch =1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    for k in range(0, (len(text_from) // batch_size) * batch_size, batch_size):\n",
    "        batch_x, seq_x = pad_sentence_batch(X[k: k+batch_size], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(Y[k: k+batch_size], PAD)\n",
    "        predicted, loss, _ = sess.run([tf.argmax(model.logits,2), model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y,\n",
    "                                                model.X_seq_len:seq_x,\n",
    "                                                model.Y_seq_len:seq_y})\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_accuracy += check_accuracy(predicted,batch_y)\n",
    "#        print 'output:', [rev_dictionary_to[i] for i in predicted[0]]\n",
    "#        print 'input:', [rev_dictionary_to[i] for i in batch_x[0]]\n",
    "        \n",
    "    total_loss /= (len(text_from) // batch_size)\n",
    "    total_accuracy /= (len(text_from) // batch_size)\n",
    "#    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, total_loss, total_accuracy))\n",
    "    path = saver.save(sess, checkpoint_prefix, global_step=i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing good thank you PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "#after training with 200 0 epochs\n",
    "print (predict('how are you doing ?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am fine thank you PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "print (predict('how are you ?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now i PAD in miami florida PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "print (predict('where do you live?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
