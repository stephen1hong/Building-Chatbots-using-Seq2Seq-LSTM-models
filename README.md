# Building-Chatbots-using-Seq2Seq-LSTM-models
i.e., implementing an encoder-decoder RNN-based LSTM unit for a simple sequence-to-sequence question-answer task

* step 1: data preparation
* Step 2: defining a seq2seq model that has 4 components: embedding layer, encoder, decoder, cost & optimizer
* step 3; hyperparameters
* step 4: training
* step 5: evaluation

---
Reference: Python Deep Learning projects, M. Lamons, R. Kumar, A. Nagaraja
